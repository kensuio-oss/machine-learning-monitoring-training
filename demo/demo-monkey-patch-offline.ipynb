{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank marketing use case |  Monkey Patch your machine learning pipeline\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from utils import data_prep, model_performance, create_observability_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan = pd.read_csv('../data/predict/jan-data.csv')\n",
    "model = pickle.load(open('../models/model_log.cav','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now aware of the potential datastrophes and failures that may happen in your model pipelines in Production. As a data scientist wanting to have more confidence in those pipelines, you have decided to prevent those failures by logging the events of your scripts. \n",
    "\n",
    "In this notebook, we will see how a logging library can be included upstream (from the creation of the dataset to the predict of the model) to avoid any `datastrophe` to happen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The data extraction script\n",
    "\n",
    "The logging strategy wants to prevent any failure when you run the model. To be complete, you need to trace and monitor your dataflow as soon as possible. In the following cell, take a moment to acquaint yourself with the code we will run and log in the exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_info = pd.read_csv('../data/predict/may-customers-data.csv')\n",
    "contact_info = pd.read_csv('../data/predict/may-contact-data.csv')\n",
    "business_info = pd.read_csv('../data/predict/may-business-data.csv')\n",
    "\n",
    "customer360 = customers_info.merge(contact_info,on='id')\n",
    "\n",
    "may = pd.merge(customer360,business_info)\n",
    "\n",
    "may.to_csv('../data/predict/may-data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to log the events of the script?\n",
    "\n",
    "In this notebook, we will show you how you can simply log all events in the notebook in order to track the lineage and enable AI observability. \n",
    "\n",
    "We will use an open source library provided by Kensu. The library will monkey patch the common python libraries such as pandas and sklearn in order to keep records of what happens in the script. With this library, we will be able to extract the following information from the notebook:\n",
    "- the data sources used and their schemas\n",
    "- the lineages (also called data flow or data map) among those data sources\n",
    "- the statistics and metrics of the data sources in that context \n",
    "\n",
    "To import the library, you need to add the kensu prefix to the library we need to monkeypatch:\n",
    "\n",
    "$$\\text{pandas} \\rightarrow  \\text{kensu.pandas}$$\n",
    "\n",
    "Once imported, you can initialize the client with the `KensuProvider` object. Several parameters are available and you can find the list in the user documentation. \n",
    "\n",
    "The `Context` of the application is defined by its `process_name` (an identifier for your application), `project_names`(where the application is running), and `environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kensu.utils.kensu_provider import KensuProvider\n",
    "kensu = KensuProvider().initKensu(process_name='Exercise 3',\n",
    "                            user_name='Sammy', \n",
    "                            code_location='https://gitlab.example.com', \n",
    "                            init_context=True, \n",
    "                            project_names=['O-Reilly'], \n",
    "                            environment=\"Production\",\n",
    "                            report_to_file=True,\n",
    "                            offline_file_name='log_pandas_example.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example of a wrapped function:\n",
    "``` python\n",
    "def wrap_pandas_get_dummies(method):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        kensu = KensuProvider().instance()\n",
    "        df_result = method(*args, **kwargs)\n",
    "        df = args[0] # see get_dummies definition (first arg is `data`)\n",
    "\n",
    "        orig_ds = kensu.extractors.extract_data_source(df)\n",
    "        orig_sc = kensu.extractors.extract_schema(orig_ds, df)\n",
    "        result_ds = kensu.extractors.extract_data_source(df_result)\n",
    "        result_sc = kensu.extractors.extract_schema(result_ds, df_result)\n",
    "\n",
    "        col_dest = [k.name for k in result_sc.pk.fields]\n",
    "        col_orig = [k.name for k in orig_sc.pk.fields]\n",
    "        prefix = kwargs['prefix'] if 'prefix' in kwargs else None\n",
    "        prefix_sep = kwargs['prefix_sep'] if 'prefix_sep' in kwargs else \"_\"\n",
    "        columns = kwargs['columns'] if 'columns' in kwargs else None\n",
    "\n",
    "        for col in col_dest:\n",
    "            if col in col_orig:\n",
    "                kensu.add_dependencies_mapping(result_sc.to_guid(), col, orig_sc.to_guid(), col)\n",
    "            else:\n",
    "                origin_col = col.split(prefix_sep)[0]\n",
    "                if prefix:\n",
    "                    if isinstance(prefix,list):\n",
    "                        index = prefix.index(origin_col)\n",
    "                        origin_col = columns[index]\n",
    "                kensu.add_dependencies_mapping(result_sc.to_guid(), col, orig_sc.to_guid(), origin_col)\n",
    "\n",
    "        return df_result\n",
    "\n",
    "    wrapper.__doc__ = method.__doc__\n",
    "    return wrapper\n",
    "\n",
    "get_dummies = wrap_pandas_get_dummies(pd.get_dummies)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Integrate the library in the following code and inspect the result \n",
    "\n",
    "- *Note:* the client has been already initialized in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "\n",
    "import kensu.pandas as pd\n",
    "\n",
    "customers_info = pd.read_csv('../data/predict/may-customers-data.csv')\n",
    "contact_info = pd.read_csv('../data/predict/may-contact-data.csv')\n",
    "business_info = pd.read_csv('../data/predict/may-business-data.csv')\n",
    "\n",
    "customer360 = customers_info.merge(contact_info,on='id')\n",
    "may = pd.merge(customer360,business_info)\n",
    "may.to_csv('../data/predict/may-data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the piece of code you've run is available in the `log_pandas_example.log` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat log_pandas_example.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is not easy to understand by itself. Fortunately, the `create_observability_report` function from the utils module will help you to generate documentation based on the log file. \n",
    "\n",
    "The function takes 2 arguments, the first one is the `log file` path, and the second is the name of the document with a `.pdf` extention that you want to generate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_observability_report('log_pandas_example.log','Report-example.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well done!\n",
    "\n",
    "A new observability report is now available in your working directory. Take a moment to explore it. What more would you like to log? \n",
    "\n",
    "In the next exercise, we will see how you can activate those reports in order to have SLA and SLO in your AI pipelines.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
